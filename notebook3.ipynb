{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc759b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "module = os.path.abspath('C:/Users\\\\thinkpad\\\\OneDrive\\\\Data-Science\\\\Workstress\\\\Mods')\n",
    "if module not in sys.path:\n",
    "    sys.path.append(module)\n",
    "from DataManager import DataManager\n",
    "from Demo import Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2766a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.compute_features();\n",
    "manager.compute_features_stress();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0506a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have\", len(manager.SUBJECTS), \" subjects\")\n",
    "\n",
    "for feature in manager.FEATURES.keys():\n",
    "    print(\"there are \", len(manager.FEATURES[feature]), \" values for \", feature)\n",
    "#     print(manager.FEATURES[feature][3277:3288])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60c511",
   "metadata": {},
   "source": [
    "lets reshape data such that there are [samples, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples, features = [N, 15]\n",
    "\n",
    "X1 = []\n",
    "X2 = []\n",
    "for i in range(0,  len(manager.FEATURES['a_mean'])):\n",
    "    X1.append([manager.FEATURES['a_mean'][i], manager.FEATURES['a_std'][i], manager.FEATURES['a_maxx'][i], manager.FEATURES['a_maxy'][i], manager.FEATURES['a_maxz'][i],\\\n",
    "                  manager.FEATURES['e_max'][i], manager.FEATURES['e_min'][i], manager.FEATURES['e_mean'][i], manager.FEATURES['e_range'][i], manager.FEATURES['e_std'][i],\\\n",
    "                  manager.FEATURES['t_max'][i], manager.FEATURES['t_min'][i], manager.FEATURES['t_mean'][i], manager.FEATURES['t_range'][i], manager.FEATURES['t_std'][i]])\n",
    "print(np.shape(X1))\n",
    "\n",
    "for i in range(0,  len(manager.STRESS_FEATURES['a_mean'])):\n",
    "    X2.append([manager.STRESS_FEATURES['a_mean'][i], manager.STRESS_FEATURES['a_std'][i], manager.STRESS_FEATURES['a_maxx'][i], manager.STRESS_FEATURES['a_maxy'][i], manager.STRESS_FEATURES['a_maxz'][i],\\\n",
    "                  manager.STRESS_FEATURES['e_max'][i], manager.STRESS_FEATURES['e_min'][i], manager.STRESS_FEATURES['e_mean'][i], manager.STRESS_FEATURES['e_range'][i], manager.STRESS_FEATURES['e_std'][i],\\\n",
    "                  manager.STRESS_FEATURES['t_max'][i], manager.STRESS_FEATURES['t_min'][i], manager.STRESS_FEATURES['t_mean'][i], manager.STRESS_FEATURES['t_range'][i], manager.STRESS_FEATURES['t_std'][i]] )\n",
    "print(np.shape(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(manager.FEATURES['a_mean'][100:110])\n",
    "# print(manager.FEATURES['a_std'][400:410])\n",
    "# print(manager.FEATURES['a_maxx'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=8, inter_op_parallelism_threads=8, )\n",
    "\n",
    "# tf.set_random_seed(1)\n",
    "\n",
    "# session = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f2fd2",
   "metadata": {},
   "source": [
    "## Initialize two arrays for the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec811e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 is the baseline data\n",
    "# x2 is the stress data\n",
    "\n",
    "y1 = [0] * len(X1)\n",
    "y2 = [1] * len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y1))\n",
    "print(len(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadf01b",
   "metadata": {},
   "source": [
    "Lets concatenate the data between baseline and stress so that we can split it into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37baa929",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X1, X2), axis=0)\n",
    "print(np.shape(X))\n",
    "\n",
    "y = np.concatenate((y1,y2), axis=0)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a9854",
   "metadata": {},
   "source": [
    "## Splitting the data up in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f537e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe3167",
   "metadata": {},
   "source": [
    "Scaling the data so we can get better results.\n",
    "\n",
    "   1. fit the scaler to the training data (fit_transform)\n",
    "   2. transform training data with the scaler\n",
    "   3. fit the model with transformed data\n",
    "   4. transform test data with the scaler\n",
    "   5. predict using model and output of (4)\n",
    "\n",
    "It will be better to have our data scaled between 0 and 1, lets normalize the data that way now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4ee208",
   "metadata": {},
   "source": [
    "The input_shape should be (number of sequences=?, time_steps=None, features=15) and the target should be (number of sequences, time_steps, targets). Since our data already has implicit timesteps from the feature calculations, we will set time_steps = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9783e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8dc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 15\n",
    "num_features = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[-]Building the LSTM network...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=True))\n",
    "# model.add(LSTM(16, input_shape=(1, 15), dropout=0.35, recurrent_dropout=0.35, return_sequences=True))\n",
    "model.add(LSTM(num_neurons, input_shape=(1, num_features), return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4113ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "print(\"inputs: \" , model.input_shape)\n",
    "print(\"outputs: \", model.output_shape)\n",
    "print(\"actual inputs: \", np.shape(X_train))\n",
    "print(\"actual outputs: \", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65517366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training LSTM...')\n",
    "\n",
    "batch_size = 2 # I think I want to use batch_size = 1\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model to disc\n",
    "json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419aa5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# Load the model of interest\n",
    "json_file = open('model.json', 'r')\n",
    "json = json_file.read()\n",
    "json_file.close()\n",
    "model_from_disc = model_from_json(json)\n",
    "model_from_disc.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7709a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.05)\n",
    "model_from_disc.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "score, acc = model_from_disc.evaluate(X_test , y_test, batch_size=batch_size)\n",
    "print('score:', score)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model_from_disc.predict(X_test, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a368cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred>0.5] = 1 \n",
    "y_pred[y_pred<=0.5] = 0 \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5d5ce",
   "metadata": {},
   "source": [
    "The LSTM-based network architecture with a single hidden layer has achieved a validation accuracy of around 97.7% after 5 epochs, which is quite impressive. Even with just one epoch, the model has shown validation accuracy results ranging from 80% to 92%. Training each epoch takes about 70 seconds without GPU acceleration. Moreover, with less modalities and features, the model surpasses the WESAD quoted results for accuracy and F1 score after just 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45751302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
